{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM AVT Stable Diffusion**\n",
        "This is a demo (intended to be run in Google Colab, with GPU enabled) of the audio-to-image models from our soon to be published research\n",
        "\n",
        "## ***Testing chatbots on the creation of encoders for audio conditioned image generation***\n",
        "\n",
        "by Jorge E. LeÃ³n (Universidad Adolfo IbÃ¡nez (UAI), Santiago, Chile) and Miguel Carrasco (Universidad Diego Portales (UDP), Santiago, Chile)."
      ],
      "metadata": {
        "id": "njRwrjOWoTng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial configuration (must be run before anything else) â˜"
      ],
      "metadata": {
        "id": "lzoodf5FpFnr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBDaaSTxFLwh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown First we download and load everything we need (this may take a few minutes and you need to run it two times).\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "flag_file = './flag.txt'\n",
        "if (not os.path.exists(flag_file)):\n",
        "  # Libraries.\n",
        "  !pip install diffusers==0.11.1\n",
        "  !pip install transformers scipy ftfy accelerate torchview\n",
        "  !pip install --upgrade diffusers transformers\n",
        "  !pip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "  # Audio encoders.\n",
        "  !git clone https://github.com/Jorvan758/A-SD-Alt.git\n",
        "  # Hugging Face.\n",
        "  !mkdir -p ~/.huggingface\n",
        "  # @markdown Here you must paste your Hugging Face token (**and don't share that token publicly**).\n",
        "  HUGGINGFACE_TOKEN = '' # @param {type:\"string\"}\n",
        "  !echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n",
        "  # Final touches.\n",
        "  with open(flag_file, 'w') as opened_file: opened_file.write('.')\n",
        "  clear_output()\n",
        "\n",
        "print('Loading libraries...')\n",
        "import torch, gc, torchaudio\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchview import draw_graph\n",
        "from scipy.io.wavfile import read\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "selected_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def clean_memory():\n",
        "  gc.collect()\n",
        "  if (selected_device == 'cuda'): torch.cuda.empty_cache()\n",
        "parameters_folder = './A-SD-Alt/Models/'\n",
        "sliced_models = './A-SD-Alt/Modelos_trozados/'\n",
        "if (not os.path.exists(parameters_folder)): os.mkdir(parameters_folder)\n",
        "\n",
        "final_files = list(set([final_file[:final_file.find('.part')] for final_file in os.listdir(sliced_models)]))\n",
        "parts_per_file = {final_file: [a_file for a_file in os.listdir(sliced_models) if final_file in a_file] for final_file in final_files}\n",
        "for part in parts_per_file: parts_per_file[part].sort()\n",
        "for final_file in final_files:\n",
        "  if (not os.path.exists(parameters_folder+final_file)):\n",
        "    print(f'Working on {final_file}...')\n",
        "    with open(parameters_folder+final_file, 'wb') as whole_out:\n",
        "      for part in parts_per_file[final_file]:\n",
        "        print(f'  ({part})')\n",
        "        with open(sliced_models+part, 'rb') as part_in:\n",
        "          data = part_in.read()\n",
        "          whole_out.write(data)\n",
        "print('All models restored.')\n",
        "\n",
        "# Ours.\n",
        "class HumanEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HumanEncoder, self).__init__()\n",
        "    ###\n",
        "    self.usar_sesgos = True\n",
        "    self.activacionSiLU = torch.nn.SiLU()\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.convolucion1d1 = torch.nn.Conv1d(1, 2, 25, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion1 = torch.nn.LayerNorm(16000)\n",
        "    self.convolucion1d2 = torch.nn.Conv1d(2, 4, 25, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion2 = torch.nn.LayerNorm(16000)\n",
        "    self.adaptador1 = torch.nn.Conv1d(1, 4, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.convolucion1d3 = torch.nn.Conv1d(4, 8, 5, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion3 = torch.nn.LayerNorm(16000)\n",
        "    self.convolucion1d4 = torch.nn.Conv1d(8, 16, 5, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion4 = torch.nn.LayerNorm(16000)\n",
        "    self.adaptador2 = torch.nn.Conv1d(4, 16, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.convolucion1d5 = torch.nn.Conv1d(16, 32, 3, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion5 = torch.nn.LayerNorm(16000)\n",
        "    self.convolucion1d6 = torch.nn.Conv1d(32, 64, 3, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion6 = torch.nn.LayerNorm(16000)\n",
        "    self.adaptador3 = torch.nn.Conv1d(16, 64, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.convolucion1d7 = torch.nn.Conv1d(64, 77, 3, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.normalizacion7 = torch.nn.LayerNorm(16000)\n",
        "    self.adaptador4 = torch.nn.Conv1d(64, 77, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.promedio = torch.nn.AdaptiveAvgPool1d(768)\n",
        "    self.linealprom = torch.nn.Linear(768, 768, bias=self.usar_sesgos)\n",
        "    self.adaptadorprom = torch.nn.Conv1d(77, 77, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.maximo = torch.nn.AdaptiveMaxPool1d(768)\n",
        "    self.linealmax = torch.nn.Linear(768, 768, bias=self.usar_sesgos)\n",
        "    self.adaptadormax = torch.nn.Conv1d(77, 77, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    self.minimo = torch.nn.AdaptiveMaxPool1d(768)\n",
        "    self.linealmin = torch.nn.Linear(768, 768, bias=self.usar_sesgos)\n",
        "    self.adaptadormin = torch.nn.Conv1d(77, 77, kernel_size=1, stride=1, padding='same', bias=self.usar_sesgos)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d1.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d2.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d3.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d4.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d5.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d6.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.convolucion1d7.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.adaptador1.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.adaptador2.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.adaptador3.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.adaptador4.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.linealprom.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.linealmax.weight)\n",
        "    torch.nn.init.kaiming_normal_(self.linealmin.weight)\n",
        "    ###\n",
        "  def forward(self, x):\n",
        "    x = x.view((-1,1,16000))/32767\n",
        "    ###\n",
        "    residual1 = x\n",
        "    x = self.convolucion1d1(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion1(x)\n",
        "    x = self.convolucion1d2(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion2(x)\n",
        "    x = self.dropout(x)\n",
        "    x += self.adaptador1(residual1)\n",
        "    residual2 = x\n",
        "    x = self.convolucion1d3(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion3(x)\n",
        "    x = self.convolucion1d4(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion4(x)\n",
        "    x = self.dropout(x)\n",
        "    x += self.adaptador2(residual2)\n",
        "    residual3 = x\n",
        "    x = self.convolucion1d5(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion5(x)\n",
        "    x = self.convolucion1d6(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion6(x)\n",
        "    x = self.dropout(x)\n",
        "    x += self.adaptador3(residual3)\n",
        "    residual4 = x\n",
        "    x = self.convolucion1d7(x)\n",
        "    x = self.activacionSiLU(x)\n",
        "    x = self.normalizacion7(x)\n",
        "    x = self.dropout(x)\n",
        "    x += self.adaptador4(residual4)\n",
        "    xprom = self.promedio(x)\n",
        "    xmax = self.maximo(x)\n",
        "    xmin = -self.minimo(-x)\n",
        "    residualprom = xprom\n",
        "    residualmax = xmax\n",
        "    residualmin = xmin\n",
        "    xprom = self.linealprom(xprom)\n",
        "    xmax = self.linealmax(xmax)\n",
        "    xmin = self.linealmin(xmin)\n",
        "    x = xprom + xmax + xmin\n",
        "    x += self.adaptadorprom(residualprom) + self.adaptadormin(residualmin) + self.adaptadormax(residualmax)\n",
        "    ###\n",
        "    assert (x.shape[1:] == (77, 768)), f\"Expected shape (-1, 77, 768), but got {x.shape}.\"\n",
        "    return x\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad(): return self.forward(x)\n",
        "  def load_parameters(self):\n",
        "    self.load_state_dict(torch.load(f'{parameters_folder}Human.pth', weights_only=False, map_location=torch.device(selected_device)))\n",
        "# ChatGPT o3-mini.\n",
        "class ChatgptEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ChatgptEncoder, self).__init__()\n",
        "    ###\n",
        "    self.num_patches = 77\n",
        "    self.patch_size = 208\n",
        "    self.proj = torch.nn.Conv1d(\n",
        "      in_channels=1,\n",
        "      out_channels=768,\n",
        "      kernel_size=self.patch_size,\n",
        "      stride=self.patch_size\n",
        "    )\n",
        "    self.pos_embed = torch.nn.Parameter(torch.zeros(self.num_patches, 768))\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "      d_model=768,\n",
        "      nhead=12,\n",
        "      dropout=0.1,\n",
        "      activation='gelu'\n",
        "    )\n",
        "    self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    ###\n",
        "  def forward(self, x):\n",
        "    x = x.view((-1,1,16000))/32767\n",
        "    ###\n",
        "    required_length = self.num_patches * self.patch_size  # 16016\n",
        "    if x.shape[2] < required_length:\n",
        "      pad_amount = required_length - x.shape[2]\n",
        "      x = torch.nn.functional.pad(x, (0, pad_amount), mode='constant', value=0)\n",
        "    x = self.proj(x)\n",
        "    x = x.transpose(1, 2)\n",
        "    x = x + self.pos_embed\n",
        "    x = self.dropout(x)\n",
        "    x = x.transpose(0, 1)\n",
        "    x = self.transformer(x)\n",
        "    x = x.transpose(0, 1)\n",
        "    ###\n",
        "    assert (x.shape[1:] == (77, 768)), f\"Expected shape (-1, 77, 768), but got {x.shape}.\"\n",
        "    return x\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad(): return self.forward(x)\n",
        "  def load_parameters(self):\n",
        "    self.load_state_dict(torch.load(f'{parameters_folder}Chatgpt.pth', weights_only=False, map_location=torch.device(selected_device)))\n",
        "# DeepSeek-R1.\n",
        "class DeepseekEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DeepseekEncoder, self).__init__()\n",
        "    ###\n",
        "    self.conv1 = torch.nn.Conv1d(1, 768, kernel_size=208, stride=208)\n",
        "    self.positional_embedding = torch.nn.Parameter(torch.randn(1, 77, 768))\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "      d_model=768,\n",
        "      nhead=12,\n",
        "      dim_feedforward=3072,\n",
        "      dropout=0.1,\n",
        "      activation='gelu',\n",
        "      batch_first=True\n",
        "    )\n",
        "    self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
        "    ###\n",
        "  def forward(self, x):\n",
        "    x = x.view((-1,1,16000))/32767\n",
        "    ###\n",
        "    x = torch.nn.functional.pad(x, (0, 16))\n",
        "    x = self.conv1(x)\n",
        "    x = x.transpose(1, 2)\n",
        "    x += self.positional_embedding\n",
        "    x = self.transformer(x)\n",
        "    ###\n",
        "    assert (x.shape[1:] == (77, 768)), f\"Expected shape (-1, 77, 768), but got {x.shape}.\"\n",
        "    return x\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad(): return self.forward(x)\n",
        "  def load_parameters(self):\n",
        "    self.load_state_dict(torch.load(f'{parameters_folder}Deepseek.pth', weights_only=False, map_location=torch.device(selected_device)))\n",
        "# Gemini 2.5 Pro Preview 03-25.\n",
        "class GeminiEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GeminiEncoder, self).__init__()\n",
        "    ###\n",
        "    self.sample_rate = 16000\n",
        "    self.output_seq_len = 77\n",
        "    self.embed_dim = 768\n",
        "    self.n_mels = 80\n",
        "    self.n_fft = 1024\n",
        "    self.hop_length = self.sample_rate // (self.output_seq_len - 1)\n",
        "    self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "      sample_rate=self.sample_rate,\n",
        "      n_fft=self.n_fft,\n",
        "      hop_length=self.hop_length,\n",
        "      n_mels=self.n_mels,\n",
        "      center=True,\n",
        "      power=2.0\n",
        "    )\n",
        "    self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=80)\n",
        "    self.input_projection = torch.nn.Linear(self.n_mels, self.embed_dim)\n",
        "    self.positional_embedding = torch.nn.Parameter(torch.randn(1, self.output_seq_len, self.embed_dim))\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "      d_model=self.embed_dim,\n",
        "      nhead=12,\n",
        "      dim_feedforward=self.embed_dim * 4,\n",
        "      dropout=0.1,\n",
        "      activation='gelu',\n",
        "      batch_first=True,\n",
        "      norm_first=True\n",
        "    )\n",
        "    num_encoder_layers = 8\n",
        "    self.transformer_encoder = torch.nn.TransformerEncoder(\n",
        "      encoder_layer,\n",
        "      num_layers=num_encoder_layers,\n",
        "      norm=torch.nn.LayerNorm(self.embed_dim)\n",
        "    )\n",
        "    self.apply(self._init_weights)\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, (torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d)):\n",
        "      torch.nn.init.xavier_uniform_(module.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.constant_(module.bias, 0)\n",
        "    elif isinstance(module, torch.nn.LayerNorm):\n",
        "      torch.nn.init.constant_(module.bias, 0)\n",
        "      torch.nn.init.constant_(module.weight, 1.0)\n",
        "    elif isinstance(module, torch.nn.Parameter) and module.ndim > 1:\n",
        "      torch.nn.init.xavier_uniform_(module, gain=torch.nn.init.calculate_gain('relu'))\n",
        "    ###\n",
        "  def forward(self, x):\n",
        "    x = x.view((-1,1,16000))/32767\n",
        "    ###\n",
        "    if x.dim() == 3 and x.shape[1] == 1:\n",
        "      x = x.squeeze(1)\n",
        "    elif x.dim() != 2:\n",
        "      raise ValueError(f\"Unexpected input shape: {x.shape}\")\n",
        "    mel_spec = self.mel_spectrogram(x)\n",
        "    mel_spec = self.amplitude_to_db(mel_spec)\n",
        "    mean = mel_spec.mean(dim=(-1, -2), keepdim=True)\n",
        "    std = mel_spec.std(dim=(-1, -2), keepdim=True)\n",
        "    mel_spec = (mel_spec - mean) / (std + 1e-6)\n",
        "    mel_spec = mel_spec.permute(0, 2, 1)\n",
        "    x = self.input_projection(mel_spec)\n",
        "    x = x + self.positional_embedding\n",
        "    x = self.transformer_encoder(x)\n",
        "    ###\n",
        "    assert (x.shape[1:] == (77, 768)), f\"Expected shape (-1, 77, 768), but got {x.shape}.\"\n",
        "    return x\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad(): return self.forward(x)\n",
        "  def load_parameters(self):\n",
        "    self.load_state_dict(torch.load(f'{parameters_folder}Gemini.pth', weights_only=False, map_location=torch.device(selected_device)))\n",
        "# Grok 3.\n",
        "class GrokEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GrokEncoder, self).__init__()\n",
        "    ###\n",
        "    self.register_buffer('window', torch.hann_window(256))\n",
        "    self.linear = torch.nn.Linear(129, 768)\n",
        "    self.pos_emb = torch.nn.Parameter(torch.randn(77, 768))\n",
        "    layer = torch.nn.TransformerEncoderLayer(d_model=768, nhead=12, dim_feedforward=3072, activation='gelu', batch_first=True)\n",
        "    self.transformer = torch.nn.TransformerEncoder(layer, num_layers=12)\n",
        "    ###\n",
        "  def forward(self, x):\n",
        "    x = x.view((-1,1,16000))/32767\n",
        "    ###\n",
        "    x = x.squeeze(1)\n",
        "    stft = torch.stft(x, n_fft=256, hop_length=207, win_length=256, window=self.window, center=False, return_complex=True)\n",
        "    mag = torch.abs(stft)\n",
        "    log_mag = torch.log(1 + mag)\n",
        "    log_mag = log_mag.permute(0, 2, 1)\n",
        "    emb = self.linear(log_mag)\n",
        "    emb = emb + self.pos_emb\n",
        "    x = self.transformer(emb)\n",
        "    ###\n",
        "    assert (x.shape[1:] == (77, 768)), f\"Expected shape (-1, 77, 768), but got {x.shape}.\"\n",
        "    return x\n",
        "  def predict(self, x):\n",
        "    with torch.no_grad(): return self.forward(x)\n",
        "  def load_parameters(self):\n",
        "    self.load_state_dict(torch.load(f'{parameters_folder}Grok.pth', weights_only=False, map_location=torch.device(selected_device)))\n",
        "\n",
        "\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained('stable-diffusion-v1-5/stable-diffusion-v1-5', subfolder='vae')\n",
        "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
        "human_encoder = HumanEncoder().to(selected_device)\n",
        "chatgpt_encoder = ChatgptEncoder().to(selected_device)\n",
        "deepseek_encoder = DeepseekEncoder().to(selected_device)\n",
        "gemini_encoder = GeminiEncoder().to(selected_device)\n",
        "grok_encoder = GrokEncoder().to(selected_device)\n",
        "human_encoder.load_parameters()\n",
        "chatgpt_encoder.load_parameters()\n",
        "deepseek_encoder.load_parameters()\n",
        "gemini_encoder.load_parameters()\n",
        "grok_encoder.load_parameters()\n",
        "audio_encoders = {'Human': chatgpt_encoder,\n",
        "                  'ChatGPT': chatgpt_encoder,\n",
        "                  'DeepSeek': deepseek_encoder,\n",
        "                  'Gemini': gemini_encoder,\n",
        "                  'Grok': grok_encoder}\n",
        "unet = UNet2DConditionModel.from_pretrained('stable-diffusion-v1-5/stable-diffusion-v1-5', subfolder='unet')\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained('stable-diffusion-v1-5/stable-diffusion-v1-5', subfolder='scheduler')\n",
        "vae = vae.to(selected_device)\n",
        "text_encoder = text_encoder.to(selected_device)\n",
        "unet = unet.to(selected_device)\n",
        "clear_output()\n",
        "print(\"You're ready to go!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeing the architectures ðŸ“Š"
      ],
      "metadata": {
        "id": "P1wc0i1mpPD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Select the encoder you wish to display (it wil consider a batch of 1 sample as input).\n",
        "SELECTED_AUDIO_ENCODER = 'Human' # @param [\"Human\", \"ChatGPT\", \"DeepSeek\", \"Gemini\", \"Grok\"]\n",
        "\n",
        "encoder_graph = draw_graph(\n",
        "    audio_encoders[SELECTED_AUDIO_ENCODER],\n",
        "    input_size=(1, 16000),\n",
        "    device='meta',\n",
        ")\n",
        "\n",
        "encoder_graph.visual_graph"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HfrZOwoypSvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating images ðŸŽ¨"
      ],
      "metadata": {
        "id": "T_rnuFGGpTMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Now you can create images using any combination of the encoders.\n",
        "# Configurable parameters.\n",
        "# @markdown Select the image you want to give as input (leave it empty if you don't want to condition the generation with an image).\n",
        "image_path = './A-SD-Alt/Referencias/1.png' # @param {type:'string', placeholder:'(Remember to add the extension .png or .jpg)'}\n",
        "# @markdown Select the 1 second audio with 16,000 Hz you want to give as input (leave it empty if you don't want to condition the generation with an audio).\n",
        "audio_path = './A-SD-Alt/Referencias/a bonfire.wav' # @param {type:'string', placeholder:'(Remember to add the extension .wav)'}\n",
        "# @markdown Write the text you want to give as input (leave it empty if you don't want to condition the generation with a text).\n",
        "text_prompt = 'A bonfire' # @param {type:'string', placeholder:'(Only the first 77 tokens will be considered)'}\n",
        "# @markdown Select the dimensions of the output image (if you give a image as input, keep it in mind so it doesn't get distorted).\n",
        "output_height = 512 # @param {type:'integer', placeholder:'(Multiples of 8 give better results)'}\n",
        "output_width = 512 # @param {type:'integer', placeholder:'(Multiples of 8 give better results)'}\n",
        "# @markdown A higher number of denoising steps should give a sharper image, but taking a longer time.\n",
        "number_of_denoising_steps = 200 # @param {type:'integer', placeholder:'(If you are using an image as input, we suggest 200; otherwise, 100 should be enough)'}\n",
        "# @markdown A higher guidance scale implies that the output will closer resemble your input audio/text.\n",
        "guidance_scale = 10 # @param {type:'number', placeholder:'(If you are using an image as input, we suggest 10; otherwise, 7.5 should be enough)'}\n",
        "# @markdown A higher strength implies that the output will closer resemble your input image (ignore it if you aren't using an input image).\n",
        "strength = 0.7 # @param {type:'number', placeholder:'(0.7 should be enough)'}\n",
        "# @markdown Number of copies control the number of different images to generate in one pass.\n",
        "number_of_copies = 1 # @param {type:'integer', placeholder:'(If your hardware is limited, just use 1)'}\n",
        "# @markdown Change the random seed to obtain different results.\n",
        "the_random_seed = 7 # @param {type:'integer'}\n",
        "# @markdown A higher print scale displays the output images at a greater resolution.\n",
        "print_scale = 6 # @param {type:'number', placeholder:'(6 should be enough)'}\n",
        "# @markdown Select the audio encoders you want to use in the generation.\n",
        "use_human_encoder = False # @param {type:'boolean'}\n",
        "use_chatgpt_encoder = False # @param {type:'boolean'}\n",
        "use_deepseek_encoder = False # @param {type:'boolean'}\n",
        "use_gemini_encoder = False # @param {type:'boolean'}\n",
        "use_grok_encoder = False # @param {type:'boolean'}\n",
        "if (image_path != ''):\n",
        "  base_image = (np.array([Image.open(image_path).resize((output_width, output_height))]*number_of_copies)[:,:,:,:3].transpose(0, 3, 1, 2)/255 - 0.5)*2\n",
        "if (audio_path != ''):\n",
        "  base_audio = torch.tensor(np.array(read(audio_path)[1],dtype=float).astype(int), requires_grad=False, dtype=torch.float32).to(selected_device)\n",
        "generator = torch.manual_seed(the_random_seed)\n",
        "# Generating embeddings.\n",
        "conditional_embeddings = torch.zeros((number_of_copies,77,768)).to(selected_device)\n",
        "unconditional_embeddings = torch.zeros((number_of_copies,77,768)).to(selected_device)\n",
        "resulting_embeddings = torch.zeros((2*number_of_copies,77,768)).to(selected_device)\n",
        "with torch.no_grad():\n",
        "  denominator = int(use_human_encoder)+int(use_chatgpt_encoder)+int(use_deepseek_encoder)+int(use_gemini_encoder)+int(use_grok_encoder)\n",
        "  # Audio.\n",
        "  if ((audio_path != '') and (denominator > 0)):\n",
        "    the_zeros = torch.zeros((number_of_copies, 16000)).to(selected_device)\n",
        "    if (use_human_encoder):\n",
        "      conditional_embeddings += human_encoder.predict(base_audio).repeat(1, number_of_copies, 1).view(number_of_copies, 77, -1).to(selected_device)\n",
        "      unconditional_embeddings += human_encoder.predict(the_zeros).to(selected_device)\n",
        "    if (use_chatgpt_encoder):\n",
        "      conditional_embeddings += chatgpt_encoder.predict(base_audio).repeat(1, number_of_copies, 1).view(number_of_copies, 77, -1).to(selected_device)\n",
        "      unconditional_embeddings += chatgpt_encoder.predict(the_zeros).to(selected_device)\n",
        "    if (use_deepseek_encoder):\n",
        "      conditional_embeddings += deepseek_encoder.predict(base_audio).repeat(1, number_of_copies, 1).view(number_of_copies, 77, -1).to(selected_device)\n",
        "      unconditional_embeddings += deepseek_encoder.predict(the_zeros).to(selected_device)\n",
        "    if (use_gemini_encoder):\n",
        "      conditional_embeddings += gemini_encoder.predict(base_audio).repeat(1, number_of_copies, 1).view(number_of_copies, 77, -1).to(selected_device)\n",
        "      unconditional_embeddings += gemini_encoder.predict(the_zeros).to(selected_device)\n",
        "    if (use_grok_encoder):\n",
        "      conditional_embeddings += grok_encoder.predict(base_audio).repeat(1, number_of_copies, 1).view(number_of_copies, 77, -1).to(selected_device)\n",
        "      unconditional_embeddings += grok_encoder.predict(the_zeros).to(selected_device)\n",
        "    resulting_embeddings = torch.cat([unconditional_embeddings/denominator, conditional_embeddings/denominator])\n",
        "    del the_zeros, base_audio\n",
        "  # Text.\n",
        "  denominator = int(use_human_encoder or use_chatgpt_encoder or use_deepseek_encoder or use_gemini_encoder or use_grok_encoder) + 1\n",
        "  if (text_prompt != ''):\n",
        "    tokenized_prompt = tokenizer(text_prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    max_prompt_length = tokenized_prompt.input_ids.shape[-1]\n",
        "    tokenized_empty = tokenizer(['']*number_of_copies, padding='max_length', max_length=max_prompt_length, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      conditional_embeddings = text_encoder(tokenized_prompt.input_ids.to(selected_device))[0]\n",
        "      unconditional_embeddings = text_encoder(tokenized_empty.input_ids.to(selected_device))[0]\n",
        "    conditional_embeddings = conditional_embeddings.repeat(1, number_of_copies, 1)\n",
        "    conditional_embeddings = conditional_embeddings.view(number_of_copies, 77, -1)\n",
        "    resulting_embeddings += torch.cat([unconditional_embeddings, conditional_embeddings])\n",
        "    resulting_embeddings = resulting_embeddings/denominator\n",
        "    del tokenized_prompt, max_prompt_length, tokenized_empty\n",
        "  # Random.\n",
        "  if (((audio_path == '') or (denominator == 1)) and (text_prompt == '')):\n",
        "    resulting_embeddings = torch.Tensor(np.random.normal(0, 1, (2*number_of_copies, 77, 768))).to(selected_device)\n",
        "del conditional_embeddings, unconditional_embeddings, denominator\n",
        "# The image to denoise is prepared.\n",
        "scheduler.set_timesteps(number_of_denoising_steps)\n",
        "if (image_path == ''):\n",
        "  initial_step = number_of_denoising_steps\n",
        "  latent_encoding = torch.randn((number_of_copies, unet.in_channels, output_height//8, output_width//8), generator=generator).to(selected_device)*scheduler.init_noise_sigma\n",
        "else:\n",
        "  latent_encoding = vae.encode(torch.tensor(base_image).type(torch.float).to(selected_device)).latent_dist.sample()*0.18215\n",
        "  initial_step = min(int(number_of_denoising_steps*strength), number_of_denoising_steps)\n",
        "  iterations = scheduler.timesteps[-initial_step].repeat(number_of_copies).to(selected_device)\n",
        "  added_noise = torch.randn(latent_encoding.shape, generator=generator, device='cpu').to(selected_device)\n",
        "  latent_encoding = scheduler.add_noise(latent_encoding, added_noise, iterations).to(selected_device)\n",
        "  del base_image, added_noise, iterations\n",
        "clean_memory()\n",
        "# The latent image is denoised.\n",
        "for t in tqdm(scheduler.timesteps[-initial_step:]):\n",
        "  latent_image = scheduler.scale_model_input(torch.cat([latent_encoding]*2), t)\n",
        "  with torch.no_grad(): predicted_noise = unet(latent_image, t, encoder_hidden_states=resulting_embeddings).sample\n",
        "  unconditioned_predicted_noise, conditioned_predicted_noise = predicted_noise.chunk(2)\n",
        "  predicted_noise = unconditioned_predicted_noise + guidance_scale*(conditioned_predicted_noise-unconditioned_predicted_noise)\n",
        "  latent_encoding = scheduler.step(predicted_noise, t, latent_encoding).prev_sample\n",
        "del resulting_embeddings, predicted_noise, unconditioned_predicted_noise, conditioned_predicted_noise\n",
        "clean_memory()\n",
        "# The resulting image is decoded.\n",
        "latent_encoding = latent_encoding/0.18215\n",
        "with torch.no_grad(): final_images = vae.decode(latent_encoding).sample\n",
        "del latent_encoding\n",
        "clean_memory()\n",
        "final_images = (final_images / 2 + 0.5).clamp(0, 1)\n",
        "final_images = final_images.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "final_images = (final_images * 255).round().astype('uint8')\n",
        "final_images = [Image.fromarray(final_image) for final_image in final_images]\n",
        "for final_image in final_images:\n",
        "  figura = plt.figure(figsize=(print_scale*output_width/max(output_width, output_height),print_scale*output_height/max(output_width, output_height)))\n",
        "  figura.set_facecolor((56/255, 56/255, 56/255, 1))\n",
        "  plt.imshow(final_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RqyvUCnIpZB5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}